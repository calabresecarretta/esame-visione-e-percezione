{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "00f6d7f4-22e6-4706-b2ba-c795d11a27c6",
      "cell_type": "markdown",
      "source": "1 - INSTALLAZIONE DELLE LIBRERIE",
      "metadata": {}
    },
    {
      "id": "5fa41da8-d74a-45fa-8bfa-439500ef2151",
      "cell_type": "code",
      "source": "!pip install -U keras\n!pip install -q tensorflow_datasets\n!pip install -U tensorflow\n!pip install tensorflow-examples\n!pip install numpy==1.26.0\n!pip install pycocotools\n!pip install scikit-image\n!pip install pyntcloud\n!pip install torch\n!pip install tensorflow-hub",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5ad3f26-9d9a-4a76-8505-77307f380c1f",
      "cell_type": "markdown",
      "source": "2 - IMPORT ",
      "metadata": {}
    },
    {
      "id": "ea9e3122-4607-4295-82bf-8eff3c0cbb22",
      "cell_type": "code",
      "source": "import numpy as np\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow_examples.models.pix2pix import pix2pix\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nimport cv2  \nimport os\nimport ijson\nimport json\nimport urllib.request\nimport zipfile\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import SeparableConv2D, Conv2D, Conv2DTranspose, Concatenate, Input\nfrom tensorflow.keras.applications import VGG16, ResNet50\nfrom tensorflow.keras.models import Model, load_model\nfrom skimage.filters import median\nfrom skimage.morphology import disk\nimport plotly.graph_objs as go\nimport tarfile\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import backend as K\nimport keras\nfrom tkinter import Tk, filedialog\nfrom keras.saving import register_keras_serializable\nimport random\nimport shutil\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "330fb832-0b23-447b-bb9b-4a572f7bf761",
      "cell_type": "markdown",
      "source": "3 - DIRECTORY DEL PROGETTO DA SOSTITUIRE CON LA PROPRIA CARTELLA",
      "metadata": {}
    },
    {
      "id": "7fa6baa8-78b2-40a2-9edb-d8ad5b1500e1",
      "cell_type": "code",
      "source": "#DIRECTORY DEL PROGETTO DA SOSTITUIRE \nproject_dir = 'C:/Users/Tesisti/Desktop/CalabreseCarretta/Progetto'\n#VARIABILE PER ESEGUIRE L'ADDESTRAMENTO\naddestramento = False",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "904d6f2c-a536-4b44-95e9-d636b102aeee",
      "cell_type": "markdown",
      "source": "4 - DOWNLOAD DEL DATASET DI TRAINING",
      "metadata": {}
    },
    {
      "id": "335d3eaa-8f0f-4f2e-bd7d-b2e8dc728376",
      "cell_type": "code",
      "source": "def download_and_extract(url, download_dir, extract_to=None):\n    if not os.path.exists(download_dir):\n        os.makedirs(download_dir)\n    filename = url.split(\"/\")[-1]\n    file_path = os.path.join(download_dir, filename)\n\n    # Scarica il file se non esiste già\n    if not os.path.exists(file_path):\n        print(f\"Scaricando {filename}...\")\n        urllib.request.urlretrieve(url, file_path)\n        print(f\"Scaricato {filename} in {file_path}\")\n    else:\n        print(f\"{filename} esiste già, non è necessario scaricare.\")\n\n    # Estrai il file ZIP solo se necessario\n    if file_path.endswith(\".zip\") and extract_to:\n        # Verifica se la cartella è vuota\n        if not os.path.exists(extract_to):\n            if not os.path.exists(extract_to):\n                os.makedirs(extract_to)  # Crea la directory di estrazione\n            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n                print(f\"Estrazione di {filename} in {extract_to}...\")\n                # Estrai ogni file nella directory di destinazione senza creare sottocartelle duplicate\n                for member in zip_ref.namelist():\n                    member_path = os.path.join(extract_to, os.path.basename(member))\n                    if os.path.basename(member):  # Ignora eventuali directory vuote\n                        with open(member_path, \"wb\") as output_file:\n                            output_file.write(zip_ref.read(member))\n                print(f\"Estrazione completata.\")\n        else:\n            print(f\"I file in {extract_to} esistono già. Saltando l'estrazione.\")\n    return file_path\n\n\n# Percorso di salvataggio\ntrain_dataset_dir = os.path.join(project_dir, 'COCOdataset2017')\nimages_dir = os.path.join(train_dataset_dir, 'images')\nannotations_dir = os.path.join(train_dataset_dir, 'annotations')\n\ntrain_images_dir = os.path.join(images_dir, 'train2017')\nval_images_dir = os.path.join(images_dir, 'val2017')\ntest_images_dir = os.path.join(images_dir, 'test')\n# Verifica se il dataset è già presente\nif not os.path.exists(train_dataset_dir):\n    # URL per scaricare i file\n    urls = {\n        \"train_images\": \"http://images.cocodataset.org/zips/train2017.zip\",\n        \"val_images\": \"http://images.cocodataset.org/zips/val2017.zip\",\n        \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n    }\n\n    # Scarica le immagini di training\n    \n    download_and_extract(urls['train_images'], train_dataset_dir, images_dir)\n\n    # Scarica le immagini di validazione (che verranno usate come test)\n    \n    download_and_extract(urls['val_images'], train_dataset_dir, images_dir)\n\n    # Rinominare la cartella val2017 in test\n    \n    if os.path.exists(val_images_dir):\n        os.rename(val_images_dir, test_images_dir)\n        print(f\"Cartella {val_images_dir} rinominata in {test_images_dir}.\")\n\n    # Creazione del dataset di validazione (20% delle immagini di train)\n    val_images_dir = os.path.join(images_dir, 'val2017')\n    if not os.path.exists(val_images_dir):\n        os.makedirs(val_images_dir)\n\n    train_images = os.listdir(train_images_dir)\n    random.shuffle(train_images)\n    val_split_size = int(0.2 * len(train_images))\n\n    # Sposta il 20% delle immagini di train nella cartella val\n    for image in train_images[:val_split_size]:\n        shutil.move(\n            os.path.join(train_images_dir, image),\n            os.path.join(val_images_dir, image)\n        )\n    print(f\"Creato dataset di validazione in {val_images_dir} con {val_split_size} immagini.\")\n\n    # Scarica le annotazioni\n    download_and_extract(urls['annotations'], train_dataset_dir, train_dataset_dir)\n\n    # Rinomina ed organizza le annotazioni\n    annotations_zip_dir = os.path.join(train_dataset_dir, 'annotations')\n    annotations_files = os.listdir(annotations_zip_dir)\n\n    # Sposta i file .json nella cartella 'annotations'\n    for file_name in annotations_files:\n        if file_name.endswith('.json'):\n            os.rename(\n                os.path.join(annotations_zip_dir, file_name),\n                os.path.join(annotations_dir, file_name)\n            )\n\n    print(\"Scaricamento e organizzazione del dataset COCO 2017 completati!\")\nelse:\n    print(f\"Il dataset esiste già nella cartella {train_dataset_dir}. Saltando il download e l'estrazione.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "798bce64-15a3-4fd5-b4a8-740afacf93f5",
      "cell_type": "markdown",
      "source": "5 - DEFINIZIONE DELLE FUNZIONI PER CARICARE IMMAGINI E MASCHERE NEI DATASET TENSORFLOW",
      "metadata": {}
    },
    {
      "id": "b97a0633-d7b5-4a0b-b474-0c2b67429949",
      "cell_type": "code",
      "source": "dim = 128\n\ndef get_image_id_from_path(image_path, coco):\n    \"\"\" Cerca l'ID dell'immagine nel dataset COCO basato sul nome del file \"\"\"\n    file_name = os.path.basename(image_path)\n    \n    # Ottieni tutte le immagini dal dataset COCO\n    img_info_list = coco.loadImgs(coco.getImgIds())\n    \n    # Cerca l'ID immagine corrispondente al nome del file\n    for img_info in img_info_list:\n        if img_info['file_name'] == file_name:\n            return img_info['id']\n    \n    # Se l'immagine non è stata trovata, solleva un'eccezione\n    raise ValueError(f\"ID immagine non trovato per il file: {file_name}\")\n\ndef create_mask(image_id, coco):\n    annotation_ids = coco.getAnnIds(imgIds=image_id, iscrowd=False)\n    annotations = coco.loadAnns(annotation_ids)\n    #print(f\"Trovate {len(annotations)} annotazioni per l'immagine {image_id}\")\n    \n    mask = np.zeros((dim, dim), dtype=np.uint8)\n    for annotation in annotations:\n        annotation_mask = coco.annToMask(annotation)\n        resized_mask = cv2.resize(annotation_mask, (dim, dim), interpolation=cv2.INTER_NEAREST)\n        mask = np.maximum(mask, resized_mask)\n    return mask\n\ndef load_image(image_path):\n    # Carica e decodifica l'immagine\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    \n    # Ridimensiona l'immagine a dimxdim\n    image = tf.image.resize(image, [dim, dim])\n    \n    # Normalizza l'immagine nel range [0, 1]\n    image = tf.cast(image, tf.float32) / 255.0\n    \n    return image\n\ndef load_image_with_mask(image_path, coco_key):\n    \"\"\"Carica l'immagine e genera la maschera corrispondente.\"\"\"\n    # Converti il tensore in una stringa se necessario\n    image_path_str = image_path.numpy().decode('utf-8') if hasattr(image_path, 'numpy') else image_path\n    coco_key_str = coco_key.numpy().decode('utf-8') if hasattr(coco_key, 'numpy') else coco_key\n\n    # Controlla se il file esiste\n    if not os.path.exists(image_path_str):\n        print(f\"Immagine non trovata per il file: {image_path_str}\")\n        empty_image = tf.zeros((dim, dim, 3), dtype=tf.float32)\n        empty_mask = tf.zeros((dim, dim, 1), dtype=tf.float32)\n        return empty_image, empty_mask\n\n    try:\n        # Carica l'immagine\n        image = load_image(image_path_str)\n        image_id = get_image_id_from_path(image_path_str, coco_dict[coco_key_str])\n\n        # Crea la maschera associata\n        mask = create_mask(image_id, coco_dict[coco_key_str])\n\n        # Converti la maschera in un tensore di TensorFlow\n        mask = tf.convert_to_tensor(mask, dtype=tf.float32)\n        mask = tf.expand_dims(mask, axis=-1)  # Aggiungi una dimensione per renderla compatibile con (dim, dim, 1)\n\n        return image, mask\n\n    except Exception as e:\n        print(f\"Errore durante il caricamento di immagine o maschera: {e}\")\n        empty_image = tf.zeros((dim, dim, 3), dtype=tf.float32)\n        empty_mask = tf.zeros((dim, dim, 1), dtype=tf.float32)\n        return empty_image, empty_mask\n\ndef load_image_wrapper(image_path_tensor, coco_key_tensor):\n    \"\"\"Wrapper per usare tf.py_function nella pipeline TensorFlow.\"\"\"\n    image, mask = tf.py_function(func=load_image_with_mask,\n                                 inp=[image_path_tensor, coco_key_tensor],\n                                 Tout=[tf.float32, tf.float32])\n    \n    # Definisci le dimensioni e le forme per TensorFlow\n    image.set_shape([dim, dim, 3])\n    mask.set_shape([dim, dim, 1])\n    return image, mask\n\n# Definizione del livello di data augmentation\ndata_augmentation = tf.keras.Sequential([\n    layers.RandomFlip(\"horizontal_and_vertical\"),\n    layers.RandomRotation(0.2),\n    layers.RandomZoom(0.2),\n    layers.RandomContrast(0.2),\n    ])\n# Funzione di augmentazione che applica le stesse trasformazioni\ndef augment_image_and_mask(image, mask):\n    # Concatenare immagine e maschera per applicare la stessa trasformazione\n    concatenated = tf.concat([image, mask], axis=-1)\n        \n    # Applicare data augmentation su immagine + maschera concatenate\n    augmented = data_augmentation(concatenated)\n        \n    # Separare immagine e maschera\n    augmented_image = augmented[..., :-1]  # Canali immagine\n    augmented_mask = augmented[..., -1:]  # Canale maschera (interpolazione nearest)\n        \n    return augmented_image, augmented_mask\n    \n# Applicare l'augmentazione al dataset\ndef apply_augmentation_to_dataset(dataset):\n    return dataset.map(lambda image, mask: augment_image_and_mask(image, mask),\n                           num_parallel_calls=tf.data.AUTOTUNE)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "74c16d45-bdef-4d12-b066-133e2e25d25f",
      "cell_type": "markdown",
      "source": "6 - DEFINIZIONI FUNZIONI PER OTTENERE UNA PARTE DEL DATASET E PER PREPROCESSARE IL DATASET",
      "metadata": {}
    },
    {
      "id": "c6cc129f-3824-44f8-8abd-4eb15bdd36f6",
      "cell_type": "code",
      "source": "# Funzioni per ottenere un sottoinsieme del dataset e preprocessarlo\ndef get_small_dataset(dataset, fraction = 0.1):\n    dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n    small_dataset_size = int(dataset_size * fraction)\n    return dataset.take(small_dataset_size)\n\ndef preprocess_dataset(dataset):\n    def preprocess(x, y=None):\n        x = tf.cast(x, tf.float32)\n        if y is not None:  # Se y è presente, preprocessiamo anche la maschera\n            y = tf.cast(y, tf.float32)\n            return x, y\n        else:\n            return x  # Se y non è presente, restituiamo solo x\n\n    return dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ffd2b39c-30d5-48bb-aa7b-392a03632d9b",
      "cell_type": "markdown",
      "source": "7 - CREAZIONI DATASET TENSORFLOW TRAIN, VAL E TEST",
      "metadata": {}
    },
    {
      "id": "41607c39-ddc0-4056-ae5d-abdedd4af24f",
      "cell_type": "code",
      "source": "coco_annotation_path = os.path.join(train_dataset_dir, 'annotations', 'instances_train2017.json')\n# Caricare annotazioni COCO per il set di addestramento\nannotation_coco = COCO(coco_annotation_path)\n\n# Dizionario per memorizzare gli oggetti COCO\ncoco_dict = {'annotation': annotation_coco}\n\n# Directory delle immagini\ntrain_images_dir = os.path.join(train_dataset_dir, 'images', 'train2017')\nval_images_dir = os.path.join(train_dataset_dir, 'images', 'val2017')\n\n# Verifica i file nelle cartelle\ntrain_image_paths = [os.path.join(train_images_dir, fname) for fname in os.listdir(train_images_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\nval_image_paths = [os.path.join(val_images_dir, fname) for fname in os.listdir(val_images_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n\n# Creazione dei dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, ['annotation'] * len(train_image_paths)))\ntrain_dataset = train_dataset.map(lambda x, y: load_image_wrapper(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n\nval_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, ['annotation'] * len(val_image_paths)))\nval_dataset = val_dataset.map(lambda x, y: load_image_wrapper(x, y), num_parallel_calls=tf.data.AUTOTUNE)\n\n# Applica batching e prefetching\ntrain_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\n# Esegui l'addestramento su un sottoinsieme ridotto del dataset\ntrain_dataset = preprocess_dataset(train_dataset)\nval_dataset = preprocess_dataset(val_dataset)\n\nsmall_train_dataset = preprocess_dataset(get_small_dataset(train_dataset))\nsmall_val_dataset = preprocess_dataset(get_small_dataset(val_dataset))\n\ntest_image_paths = [os.path.join(test_images_dir, fname) for fname in os.listdir(test_images_dir) if fname.endswith(('.jpg', '.jpeg', '.png'))]\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(test_image_paths)\ntest_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\ntest_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset = preprocess_dataset(test_dataset)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "63c16dc3-f387-4abb-9c47-4b31ef897922",
      "cell_type": "code",
      "source": "#CANCELLARE LA CELLA\nsmall_train_dataset = preprocess_dataset(get_small_dataset(train_dataset))\nsmall_val_dataset = preprocess_dataset(get_small_dataset(val_dataset))",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2a34d750-c75b-4277-8129-b868eaa00eee",
      "cell_type": "markdown",
      "source": "8 - DEFINIZIONE DEL MODELLO CON VGG",
      "metadata": {}
    },
    {
      "id": "3472f097-ce5d-4097-a22f-e7fe8c1edc59",
      "cell_type": "code",
      "source": "# Blocco di attenzione\n\ndef attention_block_with_se(x, g, inter_channel):\n    theta_x = layers.Conv2D(inter_channel, 1)(x)\n    phi_g = layers.Conv2D(inter_channel, 1)(g)\n    concat_xg = layers.add([theta_x, phi_g])\n    relu_xg = layers.Activation('relu')(concat_xg)\n    psi = layers.Conv2D(1, 1, activation='sigmoid')(relu_xg)\n\n    # Squeeze-and-Excitation block\n    se_shape = (1, 1, x.shape[-1])\n    se = layers.GlobalAveragePooling2D()(x)\n    se = layers.Reshape(se_shape)(se)\n    se = layers.Dense(inter_channel // 2, activation='relu')(se)\n    se = layers.Dense(x.shape[-1], activation='sigmoid')(se)\n    x_se = layers.multiply([x, se])\n\n    return layers.multiply([x_se, psi])\n\n\ndef create_unet_model_with_vgg16(input_shape=(dim, dim, 3)):\n    vgg16 = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n    \n    for layer in vgg16.layers:\n        layer.trainable = False\n\n    inputs = layers.Input(shape=input_shape)\n    \n    conv1 = vgg16.get_layer('block1_conv2')(vgg16.get_layer('block1_conv1')(inputs))\n    conv1 = layers.BatchNormalization()(conv1)\n    pool1 = vgg16.get_layer('block1_pool')(conv1)\n\n    conv2 = vgg16.get_layer('block2_conv2')(vgg16.get_layer('block2_conv1')(pool1))\n    conv2 = layers.BatchNormalization()(conv2)\n    pool2 = vgg16.get_layer('block2_pool')(conv2)\n\n    conv3 = vgg16.get_layer('block3_conv3')(vgg16.get_layer('block3_conv2')(vgg16.get_layer('block3_conv1')(pool2)))\n    conv3 = layers.BatchNormalization()(conv3)\n    pool3 = vgg16.get_layer('block3_pool')(conv3)\n\n    conv4 = vgg16.get_layer('block4_conv3')(vgg16.get_layer('block4_conv2')(vgg16.get_layer('block4_conv1')(pool3)))\n    drop4 = layers.SpatialDropout2D(0.5)(conv4)\n    pool4 = vgg16.get_layer('block4_pool')(drop4)\n\n    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same', dilation_rate=2, kernel_regularizer=regularizers.l2(0.0002))(pool4)\n    conv5 = layers.BatchNormalization()(conv5)\n    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0002))(conv5)\n    conv5 = layers.BatchNormalization()(conv5)\n    drop5 = layers.SpatialDropout2D(0.5)(conv5)\n\n    up6 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(drop5))\n    merge6 = layers.concatenate([drop4, up6], axis=3)\n    conv6 = layers.DepthwiseConv2D(3, padding='same')(merge6)\n    conv6 = layers.Conv2D(512, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(conv6)\n    conv6 = layers.BatchNormalization()(conv6)\n    conv6 = layers.Dropout(0.4)(conv6)\n\n    attention6 = attention_block_with_se(conv6, up6, 512)\n\n    up7 = layers.Conv2D(512, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention6))\n    merge7 = layers.concatenate([conv3, up7], axis=3)\n    conv7 = layers.DepthwiseConv2D(3, padding='same')(merge7)\n    conv7 = layers.Conv2D(512, 1, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(conv7)\n    conv7 = layers.BatchNormalization()(conv7)\n    conv7 = layers.Dropout(0.4)(conv7)\n\n    attention7 = attention_block_with_se(conv7, up7, 512)\n\n    up8 = layers.Conv2D(256, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention7))\n    merge8 = layers.concatenate([conv2, up8], axis=3)\n    conv8 = layers.DepthwiseConv2D(3, padding='same')(merge8)\n    conv8 = layers.Conv2D(256, 1, activation='relu', kernel_regularizer=regularizers.l2(0.00005))(conv8)\n    conv8 = layers.BatchNormalization()(conv8)\n    conv8 = layers.Dropout(0.3)(conv8)\n\n    attention8 = attention_block_with_se(conv8, up8, 256)\n\n    up9 = layers.Conv2D(64, 2, activation='relu', padding='same')(layers.UpSampling2D(size=(2, 2))(attention8))\n    merge9 = layers.concatenate([conv1, up9], axis=3)\n    conv9 = layers.DepthwiseConv2D(3, padding='same')(merge9)\n    conv9 = layers.Conv2D(64, 1, activation='relu', kernel_regularizer=regularizers.l2(0.00005))(conv9)\n    conv9 = layers.BatchNormalization()(conv9)\n    conv9 = layers.Dropout(0.3)(conv9)\n\n    attention9 = attention_block_with_se(conv9, up9, 64)\n    outputs = layers.Conv2D(1, 1, activation='sigmoid')(attention9)\n\n    model = Model(inputs, outputs)\n    return model",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2b6cea70-a644-4e8b-b796-c75941947830",
      "cell_type": "markdown",
      "source": "9 - DEFINIZIONE MODELLO CON RESNET",
      "metadata": {}
    },
    {
      "id": "4db8ee60-7168-44e9-acc3-9a625dacfe4b",
      "cell_type": "code",
      "source": "@register_keras_serializable()\nclass ResizeLayer(layers.Layer):\n    def __init__(self, target_height, target_width, **kwargs):\n        super(ResizeLayer, self).__init__(**kwargs)\n        self.target_height = target_height\n        self.target_width = target_width\n\n    def call(self, inputs):\n        return tf.image.resize(inputs, size=(self.target_height, self.target_width), method='bilinear')\n\n@register_keras_serializable()\nclass AdjustDimensionsLayer(layers.Layer):\n    def __init__(self, **kwargs):\n        super(AdjustDimensionsLayer, self).__init__(**kwargs)\n\n    def call(self, inputs):\n        encoder_output, decoder_output = inputs\n        encoder_shape = tf.shape(encoder_output)\n        resized_decoder_output = tf.image.resize(\n            decoder_output, size=(encoder_shape[1], encoder_shape[2]), method='bilinear'\n        )\n        return encoder_output, resized_decoder_output\n\ndef create_unet_model_with_resnet50(input_shape=(128, 128, 3)):\n    resnet50 = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)\n    for layer in resnet50.layers:\n        layer.trainable = False\n\n    inputs = layers.Input(shape=input_shape)\n\n    # Estrarre feature intermedie da ResNet-50\n    x = resnet50.get_layer('conv1_relu')(resnet50.get_layer('conv1_conv')(inputs))\n    x = resnet50.get_layer('pool1_pool')(x)\n    conv1 = resnet50.get_layer('conv2_block3_out')(x)  # (None, 64, 64, 256)\n    conv2 = resnet50.get_layer('conv3_block4_out')(conv1)  # (None, 32, 32, 512)\n    conv3 = resnet50.get_layer('conv4_block6_out')(conv2)  # (None, 16, 16, 1024)\n    conv4 = resnet50.get_layer('conv5_block3_out')(conv3)  # (None, 8, 8, 2048)\n    drop4 = layers.SpatialDropout2D(0.5)(conv4)\n\n    # Decoder\n    up6 = layers.Conv2D(1024, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(layers.UpSampling2D(size=(2, 2))(drop4))\n    conv3, up6 = AdjustDimensionsLayer()([conv3, up6])\n    merge6 = layers.concatenate([conv3, up6], axis=3)\n    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(merge6)\n    conv6 = layers.Dropout(0.4)(conv6)\n\n    up7 = layers.Conv2D(512, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(layers.UpSampling2D(size=(2, 2))(conv6))\n    conv2, up7 = AdjustDimensionsLayer()([conv2, up7])\n    merge7 = layers.concatenate([conv2, up7], axis=3)\n    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(merge7)\n    conv7 = layers.Dropout(0.4)(conv7)\n\n    up8 = layers.Conv2D(256, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(layers.UpSampling2D(size=(2, 2))(conv7))\n    conv1, up8 = AdjustDimensionsLayer()([conv1, up8])\n    merge8 = layers.concatenate([conv1, up8], axis=3)\n    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(merge8)\n    conv8 = layers.Dropout(0.3)(conv8)\n\n    up9 = layers.Conv2D(128, 2, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(layers.UpSampling2D(size=(2, 2))(conv8))\n    up9 = ResizeLayer(128, 128)(up9) if inputs.shape[1] != up9.shape[1] else up9\n    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))(up9)\n    conv9 = layers.Dropout(0.3)(conv9)\n\n    # Ultimo layer per ottenere l'output di dimensione 128x128\n    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv9)\n\n    model = models.Model(inputs, outputs)\n    return model\n\n#model.summary()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "de4b741c-f0a6-4d92-a904-477f4556b44f",
      "cell_type": "markdown",
      "source": "10 - DEFINIZIONE DELLE METRICHE",
      "metadata": {}
    },
    {
      "id": "7b4b1637-52e9-43ab-9c5a-dfd4f011d4d9",
      "cell_type": "code",
      "source": "# Funzione personalizzata per SSIM, IoU e F1 Score e psnr\ndef ssim_metric(y_true, y_pred):\n    return tf.image.ssim(y_true, y_pred, max_val=1.0)\n\ndef iou_metric(y_true, y_pred):\n    y_pred = tf.round(y_pred)  # Arrotondamento a 0 o 1 per segmentazione binaria\n    intersection = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32))\n    union = tf.reduce_sum(tf.cast(y_true + y_pred - y_true * y_pred, tf.float32))\n    iou = (intersection + 1e-10) / (union + 1e-10)  # Evita divisioni per zero\n    return iou\n\ndef f1_score(y_true, y_pred):\n    y_pred = tf.round(y_pred)  # Arrotondamento a 0 o 1 per segmentazione binaria\n    intersection = tf.reduce_sum(y_true * y_pred)\n    precision = intersection / (tf.reduce_sum(y_pred) + 1e-10)\n    recall = intersection / (tf.reduce_sum(y_true) + 1e-10)\n    f1 = (2 * precision * recall) / (precision + recall + 1e-10)  # Evita divisioni per zero\n    return f1\n\ndef psnr_metric(y_true, y_pred):\n    return tf.image.psnr(y_true, y_pred, max_val=1.0)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c0d75f6b-b03b-4d05-93c4-038cb3f17cee",
      "cell_type": "markdown",
      "source": "11 - CREAZIONE, COMPILAZIONE E SALVATAGGIO DEL MODELLO ",
      "metadata": {}
    },
    {
      "id": "f6a5b119-3f40-47d8-aa52-9c6d293d75bb",
      "cell_type": "code",
      "source": "# Percorsi in cui salvare o da cui caricare il modello\nmodel_dir = os.path.join(project_dir, 'Modello')\nmodel_resnet_path = os.path.join(model_dir, 'modello_resnet.keras')\nmodel_vgg_path = os.path.join(model_dir, 'modello_vgg.keras')\n\n@keras.saving.register_keras_serializable()\ndef bce_dice_loss(y_true, y_pred):\n    bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n    dice_loss = 1 - (2 * tf.reduce_sum(y_true * y_pred) + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n    return bce_loss + dice_loss\n\n@keras.saving.register_keras_serializable()\ndef bce_dice_loss_with_alpha(alpha):\n    @keras.saving.register_keras_serializable()\n    def loss(y_true, y_pred):\n        bce_loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n        dice_loss = 1 - (2 * tf.reduce_sum(y_true * y_pred) + 1) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + 1)\n        return alpha * bce_loss + (1 - alpha) * dice_loss\n    return loss\n\nclass DynamicAlphaCallback(tf.keras.callbacks.Callback):\n    def __init__(self, initial_alpha=0.5, final_alpha=0.1, total_epochs=20):\n        super().__init__()\n        self.initial_alpha = initial_alpha\n        self.final_alpha = final_alpha\n        self.total_epochs = total_epochs\n        self.current_alpha = initial_alpha\n\n    def on_epoch_begin(self, epoch, logs=None):\n        # Aggiorna alpha in base all'epoca corrente\n        self.current_alpha = self.initial_alpha - ((self.initial_alpha - self.final_alpha) * (epoch / self.total_epochs))\n        print(f\"\\n[DynamicAlphaCallback] Epoch {epoch + 1}: Current alpha = {self.current_alpha:.4f}\")\n        # Aggiorna la loss nel modello\n        self.model.loss = bce_dice_loss_with_alpha(self.current_alpha)\n\n\ndynamic_alpha_callback = DynamicAlphaCallback(initial_alpha=0.5, final_alpha=0.1, total_epochs=20)\n\n#Compila il modello con un valore iniziale per alpha\ninitial_alpha = 0.5\n\ndef training(model, model_path, augmentazione=True):\n    \n    # Compilazione del modello con le metriche personalizzate\n    model.compile(\n        optimizer=AdamW(learning_rate=0.0001, weight_decay=1e-5),\n        loss=bce_dice_loss_with_alpha(initial_alpha),\n        metrics=['accuracy']\n    )\n\n    # Callback per Early Stopping e riduzione del learning rate\n    early_stopping = EarlyStopping(monitor='val_loss', \n                                   patience=5, \n                                   restore_best_weights=True)\n\n    reduce_lr = ReduceLROnPlateau(\n        monitor='val_loss',        \n        factor=0.5,               \n        patience=3,               \n        min_lr=1e-6                \n    )\n\n    if augmentazione:\n        small_train_dataset = apply_augmentation_to_dataset(small_train_dataset)\n    \n    # Addestramento del modello\n    history = model.fit(\n        small_train_dataset,\n        validation_data=small_val_dataset,\n        epochs=10,\n        verbose=1,\n        callbacks=[early_stopping, reduce_lr, dynamic_alpha_callback]\n    )\n\n    # Estrazione delle metriche dalla history\n    history_dict = history.history\n    train_loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n\n    # Salvare le metriche in un file JSON\n    metriche_history = os.path.join(os.path.dirname(model_path), 'metriche_history.json')\n    with open(metriche_history, 'w') as f:\n        json.dump({'train_loss': train_loss, 'val_loss': val_loss}, f)\n\n    # Salva il modello addestrato\n    model.save(model_path)\n\n\nif addestramento:\n    # Creazione del modello\n    model_vgg = create_unet_model_with_vgg16(input_shape=(dim, dim, 3))\n    model_resnet = create_unet_model_with_resnet50(input_shape=(dim, dim, 3))\n\n    training(model_vgg, model_vgg_path, augmentazione=True)\n    #training(model_resnet, model_resnet_path, augmentazione=False)\n       # Grafico dell'andamento di train_loss e val_loss\n    plt.figure(figsize=(10, 6))\n    plt.plot(train_loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()    \nelse: \n    # Caricamento del modello\n    model = keras.models.load_model(\n        model_vgg_path,\n        custom_objects={\n            #'bce_dice_loss_with_alpha': bce_dice_loss_with_alpha,  # Funzione di perdita principale\n            'bce_dice_loss': bce_dice_loss,                       # Versione semplice della funzione di perdita\n            'loss': bce_dice_loss_with_alpha(0.5),\n            'psnr_metric' : psnr_metric# Funzione `loss` con un valore predefinito di alpha\n        }\n    )\n\n    # model = keras.models.load_model(model_resnet_path, custom_objects={'bce_dice_loss_with_alpha': bce_dice_loss_with_alpha, \n    #                                                                    'ResizeLayer' : ResizeLayer,\n    #                                                                     'bce_dice_loss' :bce_dice_loss ,\n    #                                                                     'loss': bce_dice_loss_with_alpha(0.5),\n    #                                                                     'AdjustDimensionsLayer' : AdjustDimensionsLayer,\n    #                                                                    'psnr_metric' : psnr_metric})\n    model.trainable = False",
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e90359c8-db93-4429-85fd-d32a7468b0c9",
      "cell_type": "markdown",
      "source": "TEST PROCESSING",
      "metadata": {}
    },
    {
      "id": "34b5a558-7031-4f77-9d22-7a2e9eac5193",
      "cell_type": "markdown",
      "source": "12 - VALUTAZIONE MODELLO",
      "metadata": {}
    },
    {
      "id": "0fdc8922-b200-4d84-bc9c-17a618aae91d",
      "cell_type": "code",
      "source": "def evaluate_metrics(model, dataset, save_path='metriche_tabella.png'):\n \n    # Inizializza liste per raccogliere i valori delle metriche\n    accuracy_values = []\n    iou_values = []\n    f1_values = []\n    ssim_values = []\n    psnr_values = []\n\n    # Itera su ogni batch nel dataset di test\n    for x, y_true in dataset:\n        y_pred = model.predict(x, verbose=0)  # Previsione del modello\n\n        # Calcola le metriche per ogni batch\n        batch_accuracy = tf.reduce_mean(tf.keras.metrics.binary_accuracy(y_true, y_pred)).numpy()\n        batch_iou = iou_metric(y_true, y_pred).numpy()  # Assumi che restituisca un valore scalare\n        batch_f1 = f1_score(y_true, y_pred).numpy()  # Assumi che restituisca un valore scalare\n        batch_ssim = ssim_metric(y_true, y_pred).numpy()  # Valore medio di SSIM\n        batch_psnr = psnr_metric(y_true, y_pred).numpy()  # Valore medio di PSNR\n\n        # Aggiunge i valori delle metriche alla lista\n        accuracy_values.append(batch_accuracy)\n        iou_values.append(batch_iou)\n        f1_values.append(batch_f1)\n        ssim_values.append(batch_ssim)\n        psnr_values.append(batch_psnr)\n\n    # Calcola media e massimo per ciascuna metrica\n    metrics_summary = {\n        'accuracy': {'mean': np.mean(accuracy_values), 'max': np.max(accuracy_values)},\n        'iou': {'mean': np.mean(iou_values), 'max': np.max(iou_values)},\n        'f1_score': {'mean': np.mean(f1_values), 'max': np.max(f1_values)},\n        'ssim': {'mean': np.mean(ssim_values), 'max': np.max(ssim_values)},\n        'psnr': {'mean': np.mean(psnr_values), 'max': np.max(psnr_values)},\n    }\n\n    # Creazione di una tabella con matplotlib\n    fig, ax = plt.subplots(figsize=(10, 5))\n    ax.axis('tight')\n    ax.axis('off')\n    table_data = [\n        [\"Metric\", \"Mean\", \"Max\"],\n        [\"Accuracy\", f\"{metrics_summary['accuracy']['mean']:.4f}\", f\"{metrics_summary['accuracy']['max']:.4f}\"],\n        [\"IoU\", f\"{metrics_summary['iou']['mean']:.4f}\", f\"{metrics_summary['iou']['max']:.4f}\"],\n        [\"F1 Score\", f\"{metrics_summary['f1_score']['mean']:.4f}\", f\"{metrics_summary['f1_score']['max']:.4f}\"],\n        [\"SSIM\", f\"{metrics_summary['ssim']['mean']:.4f}\", f\"{metrics_summary['ssim']['max']:.4f}\"],\n        [\"PSNR\", f\"{metrics_summary['psnr']['mean']:.4f}\", f\"{metrics_summary['psnr']['max']:.4f}\"]\n    ]\n    table = ax.table(cellText=table_data, colLabels=None, cellLoc='center', loc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1.2, 1.2)\n\n    # Salva l'immagine\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.close(fig)\n\n    return metrics_summary\n\nmetriche_tabella_train = os.path.join(model_dir, 'metriche_tabella_train.png')\nmetriche_tabella_val = os.path.join(model_dir, 'metriche_tabella_val.png')\nmetrics = evaluate_metrics(model, small_train_dataset, save_path=metriche_tabella_train)\nmetrics = evaluate_metrics(model, small_val_dataset, save_path=metriche_tabella_val)\nprint(metrics)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "93077c79-62d1-427d-b714-20be496b5f81",
      "cell_type": "markdown",
      "source": "13 - DEFINIZIONE FUNZIONI PER OTTENERE LA DEPTH MAP E LE POINT CLOUD",
      "metadata": {}
    },
    {
      "id": "ba1ae703-530a-4d38-a941-1cbffd15b341",
      "cell_type": "code",
      "source": "# Funzione per caricare il modello MiDaS v3\ndef load_midas_model():\n    model_type = \"DPT_Large\"  # Usa DPT_Large per maggiore accuratezza\n    midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n    midas.eval()\n    transform = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").dpt_transform\n    return midas, transform\n\n# Funzione per ottenere la mappa di profondità utilizzando MiDaS v3\ndef get_depth_map(image, midas, transform):\n    img_np = (image.numpy() * 255).astype(np.uint8)\n\n    if img_np.shape[-1] == 3:\n        img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n\n    input_tensor = transform(img_np).unsqueeze(0)\n    if input_tensor.dim() == 5:\n        input_tensor = input_tensor.squeeze(1)\n\n    with torch.no_grad():\n        depth_map = midas(input_tensor).squeeze().cpu().numpy()\n\n    depth_map_normalized = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min()) * 255\n    depth_map_normalized = depth_map_normalized.astype(np.uint8)\n\n    depth_map_filtered = median(depth_map_normalized, disk(3))\n    depth_map_filtered = depth_map_filtered.astype(np.uint8)\n\n    return depth_map_filtered\n\n# Funzione per creare la nuvola di punti 3D usando la mappa di profondità\ndef image_to_3d_point_cloud(depth_map, intrinsics):\n    height, width = depth_map.shape\n    y_indices, x_indices = np.indices((height, width))\n    z_values = depth_map[y_indices, x_indices]\n\n    f_x = intrinsics['f_x']\n    f_y = intrinsics['f_y']\n    c_x = intrinsics['c_x']\n    c_y = intrinsics['c_y']\n\n    X = (x_indices - c_x)\n    Y = (y_indices - c_y)\n    Z = z_values\n\n    point_cloud_3d = np.column_stack((X.flatten(), Y.flatten(), Z.flatten()))\n    return point_cloud_3d\n\n# Parametri intrinseci della fotocamera\nintrinsics = {\n    'f_x': dim,  # Lunghezza focale sull'asse X\n    'f_y': dim,  # Lunghezza focale sull'asse Y\n    'c_x': 64,   # Centro ottico sull'asse X\n    'c_y': 64    # Centro ottico sull'asse Y\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dc6b6b4e-bfec-4a78-b5df-8e1387def636",
      "cell_type": "markdown",
      "source": "14 - DEFINIZIONE FUNZIONE PER VISUALIZZARE IMMAGINE, MASCHERA ORIGINALE, MASCHERA PREDETTA, DEPTH MAP, POINT CLOUD E POINT CLOUD SEGMENTATA",
      "metadata": {}
    },
    {
      "id": "9ddaf241-c273-44ec-a6bf-15960b1eef1d",
      "cell_type": "code",
      "source": "def visualize_and_save_predictions_with_depth(model, dataset, midas, transform, intrinsics, num_samples=3, max_points=100000, has_masks=True, output_dir='./output'):\n    os.makedirs(output_dir, exist_ok=True)  # Ensure output directory exists\n    dataset_iter = iter(dataset)\n    samples_shown = 0\n\n    print(\"Starting image processing and saving...\")\n\n    for batch in dataset_iter:\n        if samples_shown >= num_samples:\n            break\n\n        if has_masks:\n            images, masks = batch\n        else:\n            images = batch\n\n        for j in range(images.shape[0]):\n            if samples_shown >= num_samples:\n                break\n\n            image = images[j]\n            if has_masks:\n                mask = masks[j]\n\n            # Perform prediction\n            pred = model.predict(tf.expand_dims(image, axis=0))\n            binary_pred = (pred > 0.5).astype(np.uint8).squeeze()\n\n            # Get the depth map\n            depth_map = get_depth_map(image, midas, transform)\n\n            # Estrai la nuvola di punti 3D\n            point_cloud_3d = image_to_3d_point_cloud(depth_map, intrinsics)\n\n            # Visualize and save all in a single plot\n            try:\n                plt.figure(figsize=(20, 10))\n                \n                # Original Image\n                plt.subplot(1, 4, 1)\n                plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n                plt.title(\"Original Image\")\n                plt.axis(\"off\")\n\n                # Original Mask, if present\n                if has_masks:\n                    plt.subplot(1, 4, 2)\n                    plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n                    plt.imshow(mask.numpy().squeeze(), cmap='jet', alpha=0.5)\n                    plt.title(\"Original Mask\")\n                    plt.axis(\"off\")\n\n                # Predicted Mask\n                plt.subplot(1, 4, 3)\n                plt.imshow((image.numpy() * 255).astype(\"uint8\"))\n                plt.imshow(binary_pred, cmap='jet', alpha=0.5)\n                plt.title(\"Predicted Mask\")\n                plt.axis(\"off\")\n\n                # Depth Map\n                plt.subplot(1, 4, 4)\n                plt.imshow(depth_map, cmap='gray')\n                plt.title(\"Depth Map\")\n                plt.axis(\"off\")\n\n                # Save combined figure\n                combined_path = f\"{output_dir}/image_{samples_shown}_combined.png\"\n                plt.savefig(combined_path)\n                print(f\"Combined image saved: {combined_path}\")\n                plt.show()\n\n                 # Visualizza e salva la nuvola di punti 3D non segmentata\n                fig_original = go.Figure(data=[go.Scatter3d(\n                    x=point_cloud_3d[:, 0],\n                    y=point_cloud_3d[:, 1],\n                    z=point_cloud_3d[:, 2],\n                    mode='markers',\n                    marker=dict(\n                        size=2,\n                        color=point_cloud_3d[:, 2],\n                        colorscale='Jet',\n                        opacity=0.5\n                    )\n                )])\n                fig_original.update_layout(\n                    scene=dict(\n                        xaxis_title='X coordinates',\n                        yaxis_title='Y coordinates',\n                        zaxis_title='Depth (Z)'\n                    ),\n                    title=\"Interactive 3D Point Cloud (No Segmentation)\"\n                )\n                html_path = f\"{output_dir}/point_cloud_{samples_shown}_original.html\"\n                fig_original.write_html(html_path)\n                print(f\"Nuvola di punti originale salvata: {html_path}\")\n                fig_original.show()  # Mostra la nuvola non segmentata in Jupyter\n\n                # Ridimensiona la maschera predetta per corrispondere alla risoluzione della mappa di profondità\n                binary_pred_resized = cv2.resize(binary_pred, (depth_map.shape[1], depth_map.shape[0]), interpolation=cv2.INTER_NEAREST)\n                binary_pred_flat = binary_pred_resized.flatten()\n\n                # Colori: rosso per l'oggetto (maschera == 1), blu per il resto (maschera == 0)\n                colors = np.where(binary_pred_flat == 1, 'red', 'blue')\n\n                # Visualizza e salva la nuvola di punti 3D segmentata\n                fig_segmented = go.Figure(data=[go.Scatter3d(\n                    x=point_cloud_3d[:, 0],\n                    y=point_cloud_3d[:, 1],\n                    z=point_cloud_3d[:, 2],\n                    mode='markers',\n                    marker=dict(\n                        size=2,\n                        color=colors,\n                        opacity=0.8\n                    )\n                )])\n\n                fig_segmented.update_layout(\n                    scene=dict(\n                        xaxis_title='X coordinates',\n                        yaxis_title='Y coordinates',\n                        zaxis_title='Depth (Z)'\n                    ),\n                    title=\"Interactive 3D Point Cloud with Object Coloring\"\n                )\n\n                # Salva la nuvola segmentata come HTML\n                html_segmented_path = f\"{output_dir}/point_cloud_{samples_shown}_segmented.html\"\n                fig_segmented.write_html(html_segmented_path)\n                print(f\"Nuvola di punti segmentata salvata: {html_segmented_path}\")\n                fig_segmented.show()  # Mostra la nuvola segmentata in Jupyter\n\n                # Increment sample count\n                samples_shown += 1\n\n            except Exception as e:\n                print(f\"Error during saving and visualization: {e}\")\n\n            if samples_shown >= num_samples:\n                break",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "50b103e5-826c-4da5-a34c-f6929481b21a",
      "cell_type": "markdown",
      "source": "15 - VISUALIZZAZIONE RISULTATI TRAINING",
      "metadata": {}
    },
    {
      "id": "abb1a328-d79c-4f70-9b8c-5d2c6f94eed7",
      "cell_type": "code",
      "source": "# Carica il modello MiDaS v3\nmidas, transform = load_midas_model()\n\n# Esegui la visualizzazione con mappa di profondità e nuvola 3D\nvisualize_and_save_predictions_with_depth(model, small_train_dataset, midas, transform, intrinsics, num_samples=25, max_points=100000, has_masks=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "53ccb0b3-232c-42ab-b8b0-420a1e2c1720",
      "cell_type": "markdown",
      "source": "16 - VISUALIZZAZIONE RISULTATI DATASET DI TEST",
      "metadata": {}
    },
    {
      "id": "888cc2b0-8578-411a-a7fd-e4d84a007629",
      "cell_type": "code",
      "source": "visualize_and_save_predictions_with_depth(model, test_dataset, midas, transform, intrinsics, num_samples=30, max_points=100000, has_masks=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "347a44d0-1962-4bbf-8ccd-e4cc504560f3",
      "cell_type": "markdown",
      "source": "17 TEST CON IMMAGINE SCELTA DA PC",
      "metadata": {}
    },
    {
      "id": "d7b7e4f3-c29a-4e07-afcb-dfbd3be9f46a",
      "cell_type": "code",
      "source": "# Funzione per selezionare l'immagine dal PC\ndef select_image():\n    # Usa un file dialog per scegliere un'immagine\n    root = Tk()\n    root.withdraw()  # Nasconde la finestra principale di Tkinter\n    root.attributes(\"-topmost\", 1)  # Porta la finestra in primo piano\n    file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg;*.jpeg;*.png;*.bmp\")])\n    root.destroy()\n    \n    if not file_path:\n        print(\"Nessun file selezionato.\")\n        return None\n\n    # Carica l'immagine usando la funzione load_image\n    return load_image(file_path)\n\n# Carica l'immagine\nimage_tensor = select_image()\n\nif image_tensor is not None:\n    # Aggiungi la dimensione del batch\n    image_tensor = tf.expand_dims(image_tensor, axis=0)\n\nif image_tensor is not None:\n    # Crea un dataset simulato con una singola immagine per compatibilità con la funzione\n    dataset = tf.data.Dataset.from_tensors(image_tensor)\n\n    # Chiama la funzione di visualizzazione con questo dataset a immagine singola\n    output_dir='./output'\n    visualize_and_save_predictions_with_depth(\n        model=model,\n        dataset=dataset,\n        midas=midas,\n        transform=transform,\n        intrinsics=intrinsics,\n        num_samples=1,  # Una singola immagine\n        max_points=100000, has_masks=False\n    )\nelse:\n    print(\"Nessuna immagine è stata selezionata o caricata.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}